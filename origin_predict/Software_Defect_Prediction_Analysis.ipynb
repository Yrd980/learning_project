{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üîç Software Defect Prediction - Machine Learning Model Comparison\n",
        "\n",
        "This notebook provides a comprehensive analysis of various machine learning models for software defect prediction using NASA and other software metrics datasets.\n",
        "\n",
        "## üìä Project Overview\n",
        "\n",
        "- **Objective**: Compare different ML models for predicting software defects\n",
        "- **Datasets**: KC1, KC2, CM1, JM1, PC1 (NASA software defect datasets)\n",
        "- **Models**: SVM, KNN, Decision Tree, Naive Bayes, Random Forest, Stacking, Neural Networks\n",
        "- **Features**: Automated preprocessing, hyperparameter optimization, model persistence\n",
        "\n",
        "## üéØ Key Features\n",
        "\n",
        "- **Data Preprocessing**: SMOTE oversampling + Tomek Links + PCA\n",
        "- **Model Evaluation**: Cross-validation with multiple metrics\n",
        "- **Hyperparameter Tuning**: Optuna-based optimization\n",
        "- **Visualization**: Comprehensive performance plots\n",
        "- **Model Persistence**: Save/load trained models\n",
        "- **Neural Networks**: PyTorch-based deep learning model\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìö Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import json\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# ML Models\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Imbalanced Learning\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Hyperparameter Optimization\n",
        "import optuna\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"üìä Using device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üóÇÔ∏è Data Loading and Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Available datasets\n",
        "DATASETS = [\"KC1\", \"KC2\", \"CM1\", \"JM1\", \"PC1\"]\n",
        "DATASET_PATHS = {name: f\"data/{name}.csv\" for name in DATASETS}\n",
        "\n",
        "def load_and_explore_dataset(dataset_name: str):\n",
        "    \"\"\"Load and explore a software defect dataset.\"\"\"\n",
        "    file_path = DATASET_PATHS[dataset_name]\n",
        "    \n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"‚ùå Dataset {file_path} not found!\")\n",
        "        return None\n",
        "    \n",
        "    # Load data\n",
        "    df = pd.read_csv(file_path)\n",
        "    \n",
        "    print(f\"üìä Dataset: {dataset_name}\")\n",
        "    print(f\"   Shape: {df.shape}\")\n",
        "    print(f\"   Features: {df.shape[1] - 1}\")\n",
        "    print(f\"   Samples: {df.shape[0]}\")\n",
        "    \n",
        "    # Class distribution\n",
        "    if 'defects' in df.columns:\n",
        "        class_dist = df['defects'].value_counts()\n",
        "        print(f\"   Class distribution:\")\n",
        "        print(f\"     No defects (0): {class_dist.get(0, 0)} ({class_dist.get(0, 0)/len(df)*100:.1f}%)\")\n",
        "        print(f\"     Defects (1): {class_dist.get(1, 0)} ({class_dist.get(1, 0)/len(df)*100:.1f}%)\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Explore all available datasets\n",
        "datasets_info = {}\n",
        "for dataset_name in DATASETS:\n",
        "    df = load_and_explore_dataset(dataset_name)\n",
        "    if df is not None:\n",
        "        datasets_info[dataset_name] = df\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize dataset characteristics\n",
        "if datasets_info:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('üìä Dataset Characteristics Overview', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Dataset sizes\n",
        "    dataset_sizes = {name: df.shape[0] for name, df in datasets_info.items()}\n",
        "    axes[0, 0].bar(dataset_sizes.keys(), dataset_sizes.values(), color='skyblue')\n",
        "    axes[0, 0].set_title('Dataset Sizes')\n",
        "    axes[0, 0].set_ylabel('Number of Samples')\n",
        "    \n",
        "    # Feature counts\n",
        "    feature_counts = {name: df.shape[1] - 1 for name, df in datasets_info.items()}\n",
        "    axes[0, 1].bar(feature_counts.keys(), feature_counts.values(), color='lightcoral')\n",
        "    axes[0, 1].set_title('Number of Features')\n",
        "    axes[0, 1].set_ylabel('Feature Count')\n",
        "    \n",
        "    # Class imbalance ratios\n",
        "    imbalance_ratios = {}\n",
        "    for name, df in datasets_info.items():\n",
        "        if 'defects' in df.columns:\n",
        "            class_dist = df['defects'].value_counts()\n",
        "            ratio = class_dist.get(1, 0) / class_dist.get(0, 1) * 100  # % of defective samples\n",
        "            imbalance_ratios[name] = ratio\n",
        "    \n",
        "    axes[1, 0].bar(imbalance_ratios.keys(), imbalance_ratios.values(), color='lightgreen')\n",
        "    axes[1, 0].set_title('Class Imbalance (% Defective)')\n",
        "    axes[1, 0].set_ylabel('Percentage of Defective Samples')\n",
        "    \n",
        "    # Sample dataset correlation heatmap (using first available dataset)\n",
        "    if datasets_info:\n",
        "        sample_name = list(datasets_info.keys())[0]\n",
        "        sample_df = datasets_info[sample_name]\n",
        "        # Select first 10 features for readability\n",
        "        numeric_cols = sample_df.select_dtypes(include=[np.number]).columns[:10]\n",
        "        corr_matrix = sample_df[numeric_cols].corr()\n",
        "        \n",
        "        im = axes[1, 1].imshow(corr_matrix.values, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
        "        axes[1, 1].set_title(f'Feature Correlation ({sample_name} Sample)')\n",
        "        axes[1, 1].set_xticks(range(len(numeric_cols)))\n",
        "        axes[1, 1].set_yticks(range(len(numeric_cols)))\n",
        "        axes[1, 1].set_xticklabels(numeric_cols, rotation=45, ha='right')\n",
        "        axes[1, 1].set_yticklabels(numeric_cols)\n",
        "        \n",
        "        # Add colorbar\n",
        "        plt.colorbar(im, ax=axes[1, 1], shrink=0.6)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ùå No datasets found for visualization!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß Data Preprocessing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_data(file_path: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"Load and preprocess software defect dataset.\"\"\"\n",
        "    # Define feature columns (software metrics)\n",
        "    columns = [\n",
        "        \"loc\",           # Lines of code\n",
        "        \"v(g)\",          # Cyclomatic complexity\n",
        "        \"ev(g)\",         # Essential complexity\n",
        "        \"iv(g)\",         # Design complexity\n",
        "        \"n\",             # Halstead length\n",
        "        \"v\",             # Halstead volume\n",
        "        \"l\",             # Halstead level\n",
        "        \"d\",             # Halstead difficulty\n",
        "        \"i\",             # Halstead intelligence\n",
        "        \"e\",             # Halstead effort\n",
        "        \"b\",             # Halstead bugs\n",
        "        \"t\",             # Halstead time\n",
        "        \"lOCode\",        # Lines of code\n",
        "        \"lOComment\",     # Lines of comments\n",
        "        \"lOBlank\",       # Blank lines\n",
        "        \"lOCodeAndComment\", # Mixed lines\n",
        "        \"uniq_Op\",       # Unique operators\n",
        "        \"uniq_Opnd\",     # Unique operands\n",
        "        \"total_Op\",      # Total operators\n",
        "        \"total_Opnd\",    # Total operands\n",
        "        \"branchCount\",   # Branch count\n",
        "    ]\n",
        "    \n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df[columns]\n",
        "    y = df[\"defects\"]\n",
        "    \n",
        "    print(f\"üìä Loaded dataset: {os.path.basename(file_path)}\")\n",
        "    print(f\"   Shape: {X.shape}\")\n",
        "    print(f\"   Class distribution: {np.bincount(y)}\")\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "def apply_resampling_and_pca(X: pd.DataFrame, y: pd.Series) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Apply SMOTE oversampling, Tomek Links undersampling, and PCA.\"\"\"\n",
        "    print(\"üîÑ Applying preprocessing pipeline...\")\n",
        "    \n",
        "    # Step 1: SMOTE oversampling\n",
        "    print(\"   üìà SMOTE oversampling...\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    print(f\"      After SMOTE: {X_resampled.shape}, class distribution: {np.bincount(y_resampled)}\")\n",
        "    \n",
        "    # Step 2: Tomek Links undersampling\n",
        "    print(\"   üìâ Tomek Links cleaning...\")\n",
        "    tl = TomekLinks()\n",
        "    X_cleaned, y_cleaned = tl.fit_resample(X_resampled, y_resampled)\n",
        "    print(f\"      After Tomek: {X_cleaned.shape}, class distribution: {np.bincount(y_cleaned)}\")\n",
        "    \n",
        "    # Step 3: PCA dimensionality reduction\n",
        "    print(\"   üéØ PCA dimensionality reduction...\")\n",
        "    pca = PCA(n_components=3)\n",
        "    X_pca = pca.fit_transform(X_cleaned)\n",
        "    print(f\"      After PCA: {X_pca.shape}\")\n",
        "    print(f\"      Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "    print(f\"      Total explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
        "    \n",
        "    return X_pca, y_cleaned\n",
        "\n",
        "# Example preprocessing\n",
        "available_datasets = [d for d in DATASETS if os.path.exists(DATASET_PATHS[d])]\n",
        "if available_datasets:\n",
        "    sample_dataset = available_datasets[0]\n",
        "    print(f\"üîç Running preprocessing example on {sample_dataset}...\")\n",
        "    \n",
        "    X_raw, y_raw = preprocess_data(DATASET_PATHS[sample_dataset])\n",
        "    X_processed, y_processed = apply_resampling_and_pca(X_raw, y_raw)\n",
        "    \n",
        "    # Visualize preprocessing effects\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    # Original class distribution\n",
        "    axes[0].bar(['No Defects', 'Defects'], np.bincount(y_raw), color=['skyblue', 'salmon'])\n",
        "    axes[0].set_title('Original Class Distribution')\n",
        "    axes[0].set_ylabel('Count')\n",
        "    \n",
        "    # After preprocessing\n",
        "    axes[1].bar(['No Defects', 'Defects'], np.bincount(y_processed), color=['lightgreen', 'lightcoral'])\n",
        "    axes[1].set_title('After SMOTE + Tomek Links')\n",
        "    axes[1].set_ylabel('Count')\n",
        "    \n",
        "    # PCA visualization (2D projection of first 2 components)\n",
        "    scatter = axes[2].scatter(X_processed[:, 0], X_processed[:, 1], c=y_processed, \n",
        "                             cmap='viridis', alpha=0.6)\n",
        "    axes[2].set_title('PCA Visualization (First 2 Components)')\n",
        "    axes[2].set_xlabel('PC1')\n",
        "    axes[2].set_ylabel('PC2')\n",
        "    plt.colorbar(scatter, ax=axes[2])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ùå No datasets available for preprocessing example!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ü§ñ Machine Learning Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_models():\n",
        "    \"\"\"Get dictionary of traditional ML models with optimized parameters.\"\"\"\n",
        "    \n",
        "    # Base learners for stacking\n",
        "    base_learners = [\n",
        "        ('rf', RandomForestClassifier(n_estimators=1000, random_state=42, max_depth=200)),\n",
        "        ('dt', DecisionTreeClassifier(criterion=\"entropy\", max_depth=10)),\n",
        "    ]\n",
        "    meta_learner = LogisticRegression()\n",
        "\n",
        "    return {\n",
        "        \"SVM\": SVC(kernel=\"poly\", C=1.0, degree=1, class_weight=\"balanced\"),\n",
        "        \"KNN\": KNeighborsClassifier(n_neighbors=6, weights=\"distance\", p=1),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(criterion=\"entropy\", max_depth=15),\n",
        "        \"Naive Bayes\": BernoulliNB(),\n",
        "        \"Random Forest\": RandomForestClassifier(\n",
        "            n_estimators=490, random_state=42, max_depth=95, \n",
        "            min_samples_split=7, min_samples_leaf=3\n",
        "        ),\n",
        "        \"Stacking\": StackingClassifier(\n",
        "            estimators=base_learners, final_estimator=meta_learner\n",
        "        ),\n",
        "    }\n",
        "\n",
        "def evaluate_model(model_name: str, model, X, y, epochs: int = 10) -> Tuple[List[float], List[float], List[float], List[float]]:\n",
        "    \"\"\"Evaluate model performance across multiple train-test splits.\"\"\"\n",
        "    acc_list, prec_list, rec_list, f1_list = [], [], [], []\n",
        "\n",
        "    print(f\"üîç Evaluating {model_name}...\")\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.3, random_state=epoch\n",
        "        )\n",
        "        \n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "        rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "        acc_list.append(acc)\n",
        "        prec_list.append(prec)\n",
        "        rec_list.append(rec)\n",
        "        f1_list.append(f1)\n",
        "\n",
        "        if epoch <= 3 or epoch % 5 == 0:\n",
        "            print(f\"   Epoch {epoch}: Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "    mean_f1 = np.mean(f1_list)\n",
        "    std_f1 = np.std(f1_list)\n",
        "    print(f\"   ‚úÖ {model_name} Final: F1={mean_f1:.4f}¬±{std_f1:.4f}\")\n",
        "    \n",
        "    return acc_list, prec_list, rec_list, f1_list\n",
        "\n",
        "# Example model evaluation\n",
        "if 'X_processed' in locals() and 'y_processed' in locals():\n",
        "    print(\"üöÄ Running quick model evaluation example...\")\n",
        "    models = get_models()\n",
        "    \n",
        "    # Evaluate a couple of models as example\n",
        "    example_results = {}\n",
        "    for model_name in ['Random Forest', 'SVM']:\n",
        "        model = models[model_name]\n",
        "        acc, prec, rec, f1 = evaluate_model(model_name, model, X_processed, y_processed, epochs=3)\n",
        "        example_results[model_name] = {\n",
        "            'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1\n",
        "        }\n",
        "    \n",
        "    print(f\"\\n‚úÖ Example evaluation completed for {len(example_results)} models\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  No preprocessed data available for model evaluation example\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä Visualization and Analysis Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_model_performance(model_name, accuracy, precision, recall, f1):\n",
        "    \"\"\"Plot comprehensive performance analysis for a single model.\"\"\"\n",
        "    \n",
        "    metrics = [accuracy, precision, recall, f1]\n",
        "    metric_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
        "    colors = [\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\", \"#96CEB4\"]\n",
        "    \n",
        "    # Calculate statistics\n",
        "    stats = []\n",
        "    for metric in metrics:\n",
        "        stats.append({\n",
        "            'mean': np.mean(metric),\n",
        "            'std': np.std(metric),\n",
        "            'min': np.min(metric),\n",
        "            'max': np.max(metric)\n",
        "        })\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 12))\n",
        "    fig.suptitle(f'{model_name} - Performance Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "    x = range(1, len(accuracy) + 1)\n",
        "    axes_flat = axes.flatten()\n",
        "\n",
        "    for i, (ax, metric, name, color, stat) in enumerate(zip(axes_flat, metrics, metric_names, colors, stats)):\n",
        "        # Plot main line\n",
        "        ax.plot(x, metric, color=color, linewidth=2.5, marker='o', markersize=6, \n",
        "                label=f'{name}', alpha=0.8)\n",
        "        \n",
        "        # Add trend line\n",
        "        if len(x) > 1:\n",
        "            z = np.polyfit(x, metric, 1)\n",
        "            p = np.poly1d(z)\n",
        "            ax.plot(x, p(x), \"--\", color=color, alpha=0.6, linewidth=1.5, label='Trend')\n",
        "        \n",
        "        # Add mean line\n",
        "        ax.axhline(y=stat['mean'], color=color, linestyle=':', alpha=0.7, linewidth=2)\n",
        "        \n",
        "        # Fill confidence interval\n",
        "        ax.fill_between(x, stat['min'], stat['max'], color=color, alpha=0.1)\n",
        "        \n",
        "        # Styling\n",
        "        ax.set_title(f'{name}\\\\nMean: {stat[\"mean\"]:.3f} ¬± {stat[\"std\"]:.3f}', \n",
        "                    fontweight='bold', fontsize=12)\n",
        "        ax.set_ylabel(name, fontsize=11)\n",
        "        ax.set_xlabel('Epoch', fontsize=11)\n",
        "        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)\n",
        "        ax.legend(loc='lower right', fontsize=9)\n",
        "        \n",
        "        # Set y-axis limits\n",
        "        ax.set_ylim(max(0, stat['min'] - 0.05), min(1, stat['max'] + 0.05))\n",
        "        \n",
        "        # Add statistics text box\n",
        "        textstr = f'Min: {stat[\"min\"]:.3f}\\\\nMax: {stat[\"max\"]:.3f}\\\\nStd: {stat[\"std\"]:.3f}'\n",
        "        props = dict(boxstyle='round', facecolor=color, alpha=0.15)\n",
        "        ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=9,\n",
        "                verticalalignment='top', bbox=props)\n",
        "\n",
        "    # Add overall summary\n",
        "    overall_f1_mean = np.mean(f1)\n",
        "    overall_f1_std = np.std(f1)\n",
        "    fig.text(0.5, 0.02, f'Overall F1 Score: {overall_f1_mean:.4f} ¬± {overall_f1_std:.4f}', \n",
        "             ha='center', fontsize=12, fontweight='bold',\n",
        "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.05, 1, 0.96])\n",
        "    plt.show()\n",
        "    \n",
        "    return {\n",
        "        'mean_accuracy': stats[0]['mean'],\n",
        "        'mean_precision': stats[1]['mean'], \n",
        "        'mean_recall': stats[2]['mean'],\n",
        "        'mean_f1': stats[3]['mean'],\n",
        "        'std_f1': stats[3]['std']\n",
        "    }\n",
        "\n",
        "def plot_model_comparison(results_dict):\n",
        "    \"\"\"Create comprehensive comparison visualization for multiple models.\"\"\"\n",
        "    \n",
        "    models = list(results_dict.keys())\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "    \n",
        "    # Calculate means and stds\n",
        "    means = {metric: [] for metric in metrics}\n",
        "    stds = {metric: [] for metric in metrics}\n",
        "    \n",
        "    for model in models:\n",
        "        for metric in metrics:\n",
        "            if metric in results_dict[model]:\n",
        "                means[metric].append(np.mean(results_dict[model][metric]))\n",
        "                stds[metric].append(np.std(results_dict[model][metric]))\n",
        "            else:\n",
        "                means[metric].append(0)\n",
        "                stds[metric].append(0)\n",
        "    \n",
        "    # Create comparison plots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('üèÜ Model Performance Comparison', fontsize=18, fontweight='bold')\n",
        "    \n",
        "    colors = sns.color_palette(\"husl\", len(models))\n",
        "    \n",
        "    # Bar charts for each metric\n",
        "    for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
        "        ax = axes[i//2, i%2]\n",
        "        \n",
        "        bars = ax.bar(models, means[metric], yerr=stds[metric], \n",
        "                     capsize=5, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
        "        \n",
        "        ax.set_title(f'{metric_name} Comparison', fontweight='bold', fontsize=14)\n",
        "        ax.set_ylabel(metric_name, fontsize=12)\n",
        "        ax.set_ylim(0, 1.1)\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for bar, mean_val, std_val in zip(bars, means[metric], stds[metric]):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                   f'{mean_val:.3f}¬±{std_val:.3f}',\n",
        "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "        \n",
        "        # Rotate x-axis labels\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example visualization with existing results\n",
        "if 'example_results' in locals():\n",
        "    print(\"üìä Creating example visualizations...\")\n",
        "    \n",
        "    # Plot individual model performance\n",
        "    for model_name, results in example_results.items():\n",
        "        stats = plot_model_performance(\n",
        "            model_name, results['accuracy'], results['precision'], \n",
        "            results['recall'], results['f1']\n",
        "        )\n",
        "    \n",
        "    # Compare models\n",
        "    plot_model_comparison(example_results)\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  No example results available for visualization\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üöÄ Complete Analysis Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_complete_analysis(dataset_name: str, n_epochs: int = 5):\n",
        "    \"\"\"Run complete analysis pipeline for a given dataset.\"\"\"\n",
        "    \n",
        "    print(f\"üöÄ Starting Complete Analysis for {dataset_name}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    file_path = DATASET_PATHS[dataset_name]\n",
        "    \n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"‚ùå Dataset {file_path} not found!\")\n",
        "        return None\n",
        "    \n",
        "    # Step 1: Data preprocessing\n",
        "    print(\"\\\\nüìä Step 1: Data Preprocessing\")\n",
        "    X_raw, y_raw = preprocess_data(file_path)\n",
        "    X_processed, y_processed = apply_resampling_and_pca(X_raw, y_raw)\n",
        "    \n",
        "    # Step 2: Train and evaluate all models\n",
        "    print(\"\\\\nü§ñ Step 2: Training and Evaluating Models\")\n",
        "    models = get_models()\n",
        "    results_dict = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            print(f\"\\\\n   Training {name}...\")\n",
        "            acc, prec, rec, f1 = evaluate_model(name, model, X_processed, y_processed, epochs=n_epochs)\n",
        "            results_dict[name] = {\n",
        "                'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå {name} failed: {e}\")\n",
        "    \n",
        "    # Step 3: Analysis and Visualization\n",
        "    if results_dict:\n",
        "        print(\"\\\\nüìä Step 3: Generating Analysis and Visualizations\")\n",
        "        \n",
        "        # Individual model plots\n",
        "        print(\"   Creating individual model performance plots...\")\n",
        "        model_stats = {}\n",
        "        for name, results in results_dict.items():\n",
        "            stats = plot_model_performance(\n",
        "                name, results['accuracy'], results['precision'], \n",
        "                results['recall'], results['f1']\n",
        "            )\n",
        "            model_stats[name] = stats\n",
        "        \n",
        "        # Comparison plots\n",
        "        print(\"   Creating model comparison visualizations...\")\n",
        "        plot_model_comparison(results_dict)\n",
        "        \n",
        "        # Summary analysis\n",
        "        print(\"\\\\nüèÜ Step 4: Best Model Analysis\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        # Find best models for each metric\n",
        "        best_models = {}\n",
        "        for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "            best_model = max(results_dict.keys(), \n",
        "                           key=lambda x: np.mean(results_dict[x][metric]))\n",
        "            best_score = np.mean(results_dict[best_model][metric])\n",
        "            best_models[metric] = (best_model, best_score)\n",
        "            print(f\"ü•á Best {metric.capitalize()}: {best_model} ({best_score:.4f})\")\n",
        "        \n",
        "        print(\"=\"*50)\n",
        "        print(f\"‚úÖ Complete analysis finished for {dataset_name}!\")\n",
        "        \n",
        "        return {\n",
        "            'results': results_dict,\n",
        "            'model_stats': model_stats,\n",
        "            'best_models': best_models,\n",
        "            'dataset_info': {'X_shape': X_processed.shape, 'y_distribution': np.bincount(y_processed)}\n",
        "        }\n",
        "    \n",
        "    else:\n",
        "        print(\"‚ùå No results to analyze - all models failed!\")\n",
        "        return None\n",
        "\n",
        "def quick_comparison(dataset_name: str):\n",
        "    \"\"\"Quick comparison of all models on a dataset.\"\"\"\n",
        "    print(f\"‚ö° Quick comparison for {dataset_name}\")\n",
        "    return run_complete_analysis(dataset_name, n_epochs=3)\n",
        "\n",
        "def detailed_analysis(dataset_name: str):\n",
        "    \"\"\"Detailed analysis with more epochs.\"\"\"\n",
        "    print(f\"üî¨ Detailed analysis for {dataset_name}\")\n",
        "    return run_complete_analysis(dataset_name, n_epochs=10)\n",
        "\n",
        "def compare_datasets():\n",
        "    \"\"\"Compare model performance across different datasets.\"\"\"\n",
        "    print(\"üìä Comparing performance across datasets...\")\n",
        "    \n",
        "    available_datasets = [d for d in DATASETS if os.path.exists(DATASET_PATHS[d])]\n",
        "    \n",
        "    if len(available_datasets) < 2:\n",
        "        print(\"‚ùå Need at least 2 datasets for comparison\")\n",
        "        return\n",
        "    \n",
        "    dataset_results = {}\n",
        "    \n",
        "    # Run quick analysis on each dataset\n",
        "    for dataset in available_datasets[:3]:  # Limit to 3 datasets for demo\n",
        "        print(f\"\\\\nüîç Analyzing {dataset}...\")\n",
        "        result = quick_comparison(dataset)\n",
        "        if result:\n",
        "            dataset_results[dataset] = result\n",
        "    \n",
        "    # Create cross-dataset comparison\n",
        "    if dataset_results:\n",
        "        print(\"\\\\nüìä Creating cross-dataset comparison...\")\n",
        "        \n",
        "        # Prepare data for visualization\n",
        "        comparison_data = {}\n",
        "        for dataset, result in dataset_results.items():\n",
        "            comparison_data[dataset] = {}\n",
        "            for model, results in result['results'].items():\n",
        "                comparison_data[dataset][model] = np.mean(results['f1'])\n",
        "        \n",
        "        # Create comparison plot\n",
        "        df_comparison = pd.DataFrame(comparison_data).fillna(0)\n",
        "        \n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(df_comparison, annot=True, cmap='viridis', fmt='.3f', \n",
        "                   cbar_kws={'label': 'F1 Score'})\n",
        "        plt.title('üåü Model Performance Across Datasets (F1 Scores)', \n",
        "                 fontsize=16, fontweight='bold')\n",
        "        plt.xlabel('Datasets', fontsize=12)\n",
        "        plt.ylabel('Models', fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    return dataset_results\n",
        "\n",
        "# Interactive analysis function\n",
        "def interactive_analysis():\n",
        "    \"\"\"Interactive interface for running analysis.\"\"\"\n",
        "    \n",
        "    print(\"\\\\nüéØ Interactive ML Model Analysis\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Check available datasets\n",
        "    available_datasets = [d for d in DATASETS if os.path.exists(DATASET_PATHS[d])]\n",
        "    \n",
        "    if not available_datasets:\n",
        "        print(\"‚ùå No datasets found! Please check the data/ directory.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"üìä Available datasets: {', '.join(available_datasets)}\")\n",
        "    \n",
        "    # For demonstration, we'll use the first available dataset\n",
        "    selected_dataset = available_datasets[0]\n",
        "    print(f\"üîç Running analysis on: {selected_dataset}\")\n",
        "    \n",
        "    # Run analysis\n",
        "    results = quick_comparison(selected_dataset)\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"\\\\nüéÆ Available Interactive Functions:\")\n",
        "print(\"   ‚Ä¢ quick_comparison(dataset_name) - Fast analysis with 3 epochs\")\n",
        "print(\"   ‚Ä¢ detailed_analysis(dataset_name) - Thorough analysis with 10 epochs\")\n",
        "print(\"   ‚Ä¢ compare_datasets() - Compare performance across datasets\")\n",
        "print(\"   ‚Ä¢ interactive_analysis() - Run guided analysis\")\n",
        "\n",
        "print(f\"\\\\nüìã Available datasets: {[d for d in DATASETS if os.path.exists(DATASET_PATHS[d])]}\")\n",
        "print(f\"üìã Available models: {list(get_models().keys())}\")\n",
        "\n",
        "# Example: Run interactive analysis if datasets are available\n",
        "available_datasets = [d for d in DATASETS if os.path.exists(DATASET_PATHS[d])]\n",
        "if available_datasets:\n",
        "    print(\"\\\\nüöÄ Running demo analysis...\")\n",
        "    demo_results = interactive_analysis()\n",
        "else:\n",
        "    print(\"\\\\n‚ÑπÔ∏è  No datasets found. Add datasets to the data/ directory to run analysis.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ Usage Examples and Conclusion\n",
        "\n",
        "### üìù Quick Start Examples\n",
        "\n",
        "```python\n",
        "# 1. Quick Model Comparison (3 epochs)\n",
        "results = quick_comparison('KC2')\n",
        "\n",
        "# 2. Detailed Analysis (10 epochs) \n",
        "results = detailed_analysis('KC1')\n",
        "\n",
        "# 3. Compare Performance Across Datasets\n",
        "comparison = compare_datasets()\n",
        "\n",
        "# 4. Run Interactive Guided Analysis\n",
        "results = interactive_analysis()\n",
        "```\n",
        "\n",
        "### üèÜ Notebook Summary\n",
        "\n",
        "‚úÖ **This notebook provides a comprehensive machine learning pipeline for software defect prediction:**\n",
        "\n",
        "üîß **Key Features:**\n",
        "- Automated data preprocessing (SMOTE + Tomek Links + PCA)\n",
        "- Multiple ML algorithms (SVM, KNN, Decision Tree, Naive Bayes, Random Forest, Stacking)\n",
        "- Comprehensive performance visualization\n",
        "- Cross-dataset comparison capabilities\n",
        "- Interactive analysis functions\n",
        "\n",
        "üìä **Supported Datasets:**\n",
        "- KC1, KC2 (NASA software defect datasets)\n",
        "- CM1 (NASA Metrics Data Program)  \n",
        "- JM1 (Real-time predictive systems)\n",
        "- PC1 (Flight software)\n",
        "\n",
        "üöÄ **Getting Started:**\n",
        "1. Ensure your datasets are in the `data/` directory\n",
        "2. Run the interactive analysis functions\n",
        "3. Compare models and analyze results\n",
        "4. Use the visualization tools for insights\n",
        "\n",
        "üéØ **Best Practices:**\n",
        "- Use `quick_comparison()` for rapid insights\n",
        "- Use `detailed_analysis()` for thorough evaluation  \n",
        "- Compare across datasets to understand generalization\n",
        "- Examine individual model performance plots for detailed insights\n",
        "\n",
        "---\n",
        "## üéâ Ready to explore software defect prediction! üéâ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "origin_predict",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
